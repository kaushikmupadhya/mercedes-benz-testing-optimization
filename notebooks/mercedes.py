# -*- coding: utf-8 -*-
"""Mercedes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19dUh7TiOOb2rz_CB6bhDo1UJTYjUq8LI
"""

#IMPORTING DATA
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error
import joblib

warnings.filterwarnings("ignore")



train_df = pd.read_csv('/content/train.csv')
test_df = pd.read_csv('/content/test.csv')

print('Shape of Train data: {}'.format(train_df.shape))
print('Shape of Test data: {}'.format(test_df.shape))

train_df.head()

test_df.head()

# Display basic information and check for missing values
print(train_df.info())
print(train_df.isnull().sum())

# Drop the ID column as it's not useful for modeling
# Safely drop the 'ID' column if it exists
if 'ID' in train_df.columns:
    train_df = train_df.drop('ID', axis=1)

if 'ID' in test_df.columns:
    test_df = test_df.drop('ID', axis=1)

def fit_and_transform_label_encoder(train_series, test_series):
    le = LabelEncoder()
    train_encoded = le.fit_transform(train_series)

    # Handle unseen labels in the test set
    test_encoded = test_series.map(lambda s: '<unknown>' if s not in le.classes_ else s)

    # Refit the LabelEncoder with an additional class for unseen labels
    le_classes = le.classes_.tolist()
    le_classes.append('<unknown>')
    le.classes_ = np.array(le_classes)

    test_encoded = le.transform(test_encoded)

    return train_encoded, test_encoded, le

# Encode categorical variables in both train and test sets
label_encoders = {}
for col in train_df.select_dtypes(include=['object']).columns:
    train_df[col], test_df[col], label_encoders[col] = fit_and_transform_label_encoder(train_df[col], test_df[col])

# Split train data into features and target
X_train = train_df.drop('y', axis=1)
y_train = train_df['y']

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(test_df)

# Train the model
model = XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42)
model.fit(X_train_scaled, y_train)

# Predict on the test set
y_test_pred = model.predict(X_test_scaled)

# Save the predictions
submission = pd.DataFrame({'ID': range(len(y_test_pred)), 'y': y_test_pred})
submission.to_csv('submission.csv', index=False)

# Plot feature importance
feature_importances = model.feature_importances_
sorted_idx = np.argsort(feature_importances)[::-1]

plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importances[sorted_idx][:20], y=X_train.columns[sorted_idx][:20])
plt.title("Top 20 Feature Importances")
plt.show()

# Suggest which features (manufacturing parameters) to optimize
top_features = X_train.columns[sorted_idx][:5]
print(f"Top features to focus on for optimization: {list(top_features)}")

